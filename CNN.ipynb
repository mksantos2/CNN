{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkqvaVl_NICy",
        "colab_type": "text"
      },
      "source": [
        "# Objetivos deste trabalho\n",
        "- Familiarizar-se com a biblioteca PyTorch\n",
        "- Definir arquiteturas MLP simples em PyTorch\n",
        "- Treinar utilizando CIFAR10, testando diferentes arquiteturas, parâmetros, funções de loss e otimizadores\n",
        "- Comparar os resultados obtidos utilizando apenas Perpceptrons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNeRYBlnNIC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnJ3PA4ENIDA",
        "colab_type": "code",
        "outputId": "678a97a5-00ca-4796-cd24-c4e3e950d6ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Carregar os datasets\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "dataset_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "dataset_valid, dataset_train = torch.utils.data.random_split(dataset_train, [5000, 45000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x97-R2fxNIDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=256)\n",
        "valid_loader = DataLoader(dataset=dataset_valid, shuffle=False, batch_size=256)\n",
        "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2kEsoD-NIDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definir a arquitetura MLP\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        #self.conv1 = torch.nn.Conv2d(1,6,5)\n",
        "        #self.conv2 = torch.nn.Conv2d(6,16,10)\n",
        "        \n",
        "        \n",
        "        #self.conv3 = torch.nn.Conv2d(16, 16, 9)\n",
        "        #self.conv4 = torch.nn.Conv2d(16,16,5)\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.pool = torch.nn.MaxPool2d(2,2)\n",
        "        \n",
        "        \n",
        "        self.fc1 = nn.Linear(32 *16 *16 , 80) # 16 * 9 * 9\n",
        "        self.fc2 = nn.Linear(80, 70)\n",
        "        self.fc3= nn.Linear(70, 50)\n",
        "        self.fc4= nn.Linear(50, 10)  \n",
        "        self.activation_function = nn.ReLU()\n",
        "        \n",
        "        self.conv2_drop = nn.Dropout2d(0.5)\n",
        "        \n",
        "        #self.conv2_bn = nn.BatchNorm2d(20)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.activation_function(self.conv1(x))\n",
        "        x = self.conv2_drop(x)\n",
        "        x = self.activation_function(self.conv2(x))\n",
        "        x = self.conv2_drop(x)\n",
        "        \n",
        "        x = self.pool(x)\n",
        "        \n",
        "        #print(x.shape)\n",
        "        \n",
        "        #x = self.activation_function(self.conv3(x))\n",
        "        #x = self.conv2_drop(x)\n",
        "        #x = self.activation_function(self.conv4(x))\n",
        "        #x = self.conv2_drop(x)\n",
        "        \n",
        "        #x = self.pool(x)\n",
        "        \n",
        "        #print(x.shape)\n",
        "        \n",
        "        x = x.view(-1, 32 *16 *16) # 15 * 9 * 9\n",
        "        x = self.activation_function(self.fc1(x))\n",
        "        x = self.activation_function(self.fc2(x))\n",
        "        x = self.activation_function(self.fc3(x))\n",
        "        x = self.activation_function(self.fc4(x))\n",
        "        #x = self.activation_function(self.fc5(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC8JZneqNIDO",
        "colab_type": "code",
        "outputId": "f35d12b6-b2ca-4aef-e0d6-946723592085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "model = CNN()\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=8192, out_features=80, bias=True)\n",
            "  (fc2): Linear(in_features=80, out_features=70, bias=True)\n",
            "  (fc3): Linear(in_features=70, out_features=50, bias=True)\n",
            "  (fc4): Linear(in_features=50, out_features=10, bias=True)\n",
            "  (activation_function): ReLU()\n",
            "  (conv2_drop): Dropout2d(p=0.5)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxXXF-iNNIDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definir otimizador e loss\n",
        "# Nota: testar outros otimizadores e funções de loss (em particular cross entropy)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#loss_fn = torch.nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhpgpfaoNIDV",
        "colab_type": "code",
        "outputId": "a6bb5b70-efd7-4ffe-a731-d189a9f0ffd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "# Realizar o treinamento aqui\n",
        "\n",
        "epochs = 100\n",
        "one_hot = torch.eye(10)\n",
        "media_losses = []\n",
        "media_ac = []\n",
        "\n",
        "\n",
        "for epoch in range(epochs): \n",
        "  \n",
        "  model.train()\n",
        "  losses = []\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for img, category in train_loader:\n",
        "        \n",
        "    #zera o gradiente no começo do batch\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    ys = model(img)\n",
        "    \n",
        "    #one_hot_category = one_hot[category] # apenas para quando for usar MSELoss\n",
        "\n",
        "    loss = loss_fn(ys, category) #one_hot_category)\n",
        "    \n",
        "    \n",
        "\n",
        "    loss.backward()\n",
        "    \n",
        "    losses.append(loss.item())\n",
        "\n",
        "    optimizer.step()\n",
        "    \n",
        "    _, predicted = torch.max(ys.data, 1) # pega o segundo argumento que retorna de torch.max\n",
        "    correct += (predicted == category).sum().item()\n",
        "    total += category.size(0)\n",
        "    \n",
        "    \n",
        "    #print(one_hot_category, category)\n",
        "    \n",
        "  \n",
        "  ac = correct/total\n",
        "  #media_ac.append(ac)\n",
        "   #print(\"epoch : {}, train loss : {:.4f}, acc_train : {:.2f}%\".format(epoch, np.mean(losses), ac))\n",
        "  \n",
        "  print(\"Época: \", epoch+1,\" \", \"Loss: \", np.mean(losses), \"ac: \", ac) # importante mencionar que está sendo printada a média dos losses\n",
        "    \n",
        "  media_losses.append(np.mean(losses)) # losses para o plot mais embaixo\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Época:  1   Loss:  2.1816225973042576 ac:  0.20113333333333333\n",
            "Época:  2   Loss:  1.9977141720327465 ac:  0.3080888888888889\n",
            "Época:  3   Loss:  1.8954184719107368 ac:  0.34835555555555553\n",
            "Época:  4   Loss:  1.8146288401701234 ac:  0.38226666666666664\n",
            "Época:  5   Loss:  1.722892396829345 ac:  0.42335555555555554\n",
            "Época:  6   Loss:  1.6644379557533697 ac:  0.4494444444444444\n",
            "Época:  7   Loss:  1.6193863275376232 ac:  0.4629333333333333\n",
            "Época:  8   Loss:  1.5801420245658269 ac:  0.47944444444444445\n",
            "Época:  9   Loss:  1.5470188226212154 ac:  0.49177777777777776\n",
            "Época:  10   Loss:  1.5067824883894487 ac:  0.5064222222222222\n",
            "Época:  11   Loss:  1.4847117201848463 ac:  0.5116444444444445\n",
            "Época:  12   Loss:  1.4618201729926197 ac:  0.5217555555555555\n",
            "Época:  13   Loss:  1.4358218495141377 ac:  0.5291555555555556\n",
            "Época:  14   Loss:  1.4170664420182055 ac:  0.5361333333333334\n",
            "Época:  15   Loss:  1.3935589593919842 ac:  0.5454222222222223\n",
            "Época:  16   Loss:  1.375180296599865 ac:  0.5477555555555556\n",
            "Época:  17   Loss:  1.3529680987650698 ac:  0.5540888888888889\n",
            "Época:  18   Loss:  1.3360715772617946 ac:  0.5616\n",
            "Época:  19   Loss:  1.3127647265791893 ac:  0.5682\n",
            "Época:  20   Loss:  1.3017251925034956 ac:  0.5732222222222222\n",
            "Época:  21   Loss:  1.2944465482776815 ac:  0.5718\n",
            "Época:  22   Loss:  1.272361588071693 ac:  0.5797777777777777\n",
            "Época:  23   Loss:  1.2624022554267536 ac:  0.5842222222222222\n",
            "Época:  24   Loss:  1.25044156644832 ac:  0.5866222222222223\n",
            "Época:  25   Loss:  1.2314401431517168 ac:  0.5938444444444444\n",
            "Época:  26   Loss:  1.2225886725566604 ac:  0.5941111111111111\n",
            "Época:  27   Loss:  1.2122608921067282 ac:  0.5986\n",
            "Época:  28   Loss:  1.1981420069932938 ac:  0.6046\n",
            "Época:  29   Loss:  1.1907294920899651 ac:  0.6040888888888889\n",
            "Época:  30   Loss:  1.1814909286119721 ac:  0.6077333333333333\n",
            "Época:  31   Loss:  1.1729930070313541 ac:  0.6104222222222222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ9ptdfY2dM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#graphs\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel(\"Epoca\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Gráfico Loss\")\n",
        "plt.plot(media_losses)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yeqVuZJNIDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Avaliar o modelo aqui (no conjunto de avaliação)\n",
        "correct = 0\n",
        "valid_losses = []\n",
        "total = 0\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, category in valid_loader:\n",
        "                y = model(images)\n",
        "\n",
        "                #one_hot_category = one_hot[category]\n",
        "                loss = loss_fn(y, category)\n",
        "\n",
        "\n",
        "                valid_losses.append(loss.item())\n",
        "\n",
        "                _, predicted = torch.max(y.data, 1)\n",
        "                correct += (predicted == category).sum().item()\n",
        "                total += category.size(0)\n",
        "      \n",
        "#ac = correct/len(valid_loader)\n",
        "ac = correct/total\n",
        "print(\"Loss_valid: \", np.mean(valid_losses), \" \", \"Accuracy_valid: \", ac)\n",
        "\n",
        "correct = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaANB3Nu1RcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testar o modelo aqui \n",
        "\n",
        "\n",
        "test_losses = []\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, category in test_loader:\n",
        "                yt = model(images)\n",
        "\n",
        "                #one_hot_category = one_hot[category]\n",
        "                loss = loss_fn(yt, category)\n",
        "\n",
        "\n",
        "                test_losses.append(loss.item())\n",
        "\n",
        "                _, predicted = torch.max(yt.data, 1)\n",
        "                correct += (predicted == category).sum().item()\n",
        "                total += category.size(0)\n",
        "\n",
        "\n",
        "\n",
        "ac = correct/total \n",
        "\n",
        "#ac = correct/len(test_loader)\n",
        "print(\"Loss_test: \", np.mean(test_losses), \" \", \"Accuracy_test: \", ac)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCcRWR6BD2A7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Resultados obtidos:\n",
        "\n",
        "# ---> MLP com 1 camada(s)(20) oculta(s) de 20 perceptrons + SGD + MSELoss + ReLU(sem particionar o dataset de treino): \n",
        "###### Loss_train = 0.08425  \n",
        "###### Loss_valid = 0       ###### Accuracy_valid = 0\n",
        "###### Loss_test  = 0.08443 ###### Accuracy_test =  0.5806\n",
        "\n",
        "\n",
        "# ---> MLP com 1 camada(s)(20) oculta(s) de 20 perceptrons + SGD + MSELoss + ReLU(particionando o dataset de treino*): \n",
        "###### Loss_train = 0.0846 \n",
        "###### Loss_valid = 0.0847 ###### Accuracy_valid = 0.2844\n",
        "###### Loss_test  = 0.0849 ###### Accuracy_test  = 0.2868\n",
        "\n",
        "\n",
        "# ---> MLP com 1 camada(s)(20) oculta(s) de 20 perceptrons + SGD + CrossEntropyLoss* + ReLU(particionando o dataset de treino):\n",
        "###### Loss_train = 1.9290 \n",
        "###### Loss_valid = 1.9698 ###### Accuracy_valid = 0.336\n",
        "###### Loss_test  = 1.9513 ###### Accuracy_test  = 0.3353\n",
        "\n",
        "\n",
        "\n",
        "# ---> MLP com 1 camada(s)(20) oculta(s) de 20 perceptrons + Adam + CrossEntropyLoss* + ReLU(particionando o dataset de treino):\n",
        "###### Loss_train = 1.9606 \n",
        "###### Loss_valid = 2.0047 ###### Accuracy_valid = 0.3042\n",
        "###### Loss_test  = 2.0014 ###### Accuracy_test  = 0.3032\n",
        "\n",
        "\n",
        "# ---> MLP com 2 camada(s) oculta(s)(50 e 20) de 20 perceptrons + Adam + CrossEntropyLoss + ReLU(particionando o dataset de treino):\n",
        "###### Loss_train = 1.7311 \n",
        "###### Loss_valid = 1.8388 ###### Accuracy_valid = 0.3738\n",
        "###### Loss_test  = 1.8464 ###### Accuracy_test  = 0.3585\n",
        "\n",
        "\n",
        "# ---> MLP com 2 camada(s) oculta(s)(50 e 20) de 20 perceptrons + SGD* + MSELoss* + ReLU(particionando o dataset de treino):\n",
        "###### Loss_train = 0.0936 \n",
        "###### Loss_valid = 0.0938 ###### Accuracy_valid = 0.18\n",
        "###### Loss_test  = 0.0934 ###### Accuracy_test  = 0.1816\n",
        "\n",
        "\n",
        "# ---> MLP com 4 camada(s) ocultas(100 80 50 e 20) de 20 perceptrons + SGD + MSELoss + ReLU(particionando o dataset de treino):\n",
        "###### Loss_train = 1.9197 \n",
        "###### Loss_valid = 1.9108 ###### Accuracy_valid = 0.3156\n",
        "###### Loss_test  = 1.9309 ###### Accuracy_test  = 0.3026\n",
        "\n",
        "\n",
        "#****** ---> MLP com 4 camada(s) ocultas(100 80 50 e 20) de 20 perceptrons + Adam* + CrossEntropyLoss* + ReLU(particionando o dataset de treino):\n",
        "###### Loss_train = 1.5833 \n",
        "###### Loss_valid = 1.9346 ###### Accuracy_valid = 0.358\n",
        "###### Loss_test  = 1.9337 ###### Accuracy_test  = 0.3708\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''testh\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImnggM95Gb8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiB6jvf0VIL6",
        "colab_type": "text"
      },
      "source": [
        "#Conclusões\n",
        "\n",
        "Depois de testar os diversos parâmetros da MLP é possível perceber a tamanha diversidade e opções para moldá-la de acordo com o domínio ou projeto, o que é muito bom comparado ao modo que faziamos no perceptron. No primeiro trabalho foi difícil e demorado de fazer cada neurônio separado e depois juntá-los, porém no final tinhamos a melhor resposta possível(tirando o fato de ajustar apenas o neta). \n",
        "\n",
        "Entretanto, na MLP, é possível alterar diversos aspectos, como a arquitetura, tipo de loss, tipo de optimizador, porém é mais fácil e rápido de implementar, graças ao pytorch. Por outro lado, se perde muito tempo achando os parâmetros certos para que a rede consiga atingir o máximo de acurácia possível(no caso esperar que o treinamento acabe). \n",
        "\n",
        "\n",
        "É preciso mencionar também que a acurácia baixou drasticamente com o particionamento dos exemplos de treinamento e avaliação. O que eu já sabia que ia acontecer, entretanto seja melhor botar mais exemplos no treinamento já que eu apenas separei alguns para a avaliação.\n",
        "\n",
        "E sobre o modelo final que escolhi como um dos melhores, foi o que possuia 4 camadas ocultas, sendo elas respectivamente de tamanhos: 100, 80, 50, 20. Eu optei em fazer uma escada como foi recomendado em aula, justamente pelo fato do aprendizado da rede neural ser como uma, onde se aprende o básico e depois irá subir até convergir para uma saída/porta. \n",
        "\n",
        "Eu comecei a usar o MSELoss mas depois de tentar todas as combinações cheguei a conclusão que o CrossEntropy é muito bom para a rede, pois observei que a Loss converge muito mais rápido do que o MSE. E também como vimos em aula o CrossEntropy é o mais recomendado para problemas de classificação.\n",
        "\n",
        "O otimizador que estou usando é o Adam, que foi muito bem combinado com o CrossEntropy.(Dados das redes célula a cima)\n",
        "\n",
        "Depois de definir qual loss e qual otimizador, testei várias arquiteturas. A partir da 3 camada oculta não dava tanto avanço nos resultados. Então optei por deixar ela pequena com 3 camadas de 80, 70 e 50, respectvamente. E os resultados foram os seguintes:\n",
        "\n",
        "\n",
        "#MLP(\n",
        "  (fc1): Linear(in_features=1024, out_features=80, bias=True)\n",
        "  (fc2): Linear(in_features=80, out_features=70, bias=True)\n",
        "  (fc3): Linear(in_features=70, out_features=50, bias=True)\n",
        "  (fc4): Linear(in_features=50, out_features=10, bias=True)\n",
        "  (activation_function): ReLU()\n",
        "#)\n",
        "\n",
        "#-----\n",
        "\n",
        "\n",
        ">Média do Loss no treinamento: 1.4947\n",
        "\n",
        "\n",
        ">Média da Loss na Avaliação:     1.8225\n",
        "\n",
        "\n",
        ">Média da Loss no Teste:            1.8141\n",
        "\n",
        "\n",
        "#-----\n",
        "\n",
        "\n",
        ">Acurácia na Avaliação:               39,82%\n",
        "\n",
        ">Acurácia no Teste:                      39,1%\n",
        "\n"
      ]
    }
  ]
}